{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1: Autoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss in class that autoencoder can automatically discover the binary code for decimal numbers. The case we consider in class is the binary code for 8 decimal numbers from 0 to 7. Recall that in this autoencoder we use a position encoding scheme (check your class notes to see what that is). \n",
    "We now construct a few autoencoders for finding binary code for 16 decimal numbers from 0 to 15. We are going to use the same position encoding scheme here.  \n",
    "For these autoencoders, the input layers and the output layers all have 16 nodes. We will use the sigmod function (i.e., f(x) = 1/(1+e^(-x))) for perceptron activation and consider the following architectures (for problems 1-4):\n",
    "\n",
    "1.\tA 3-layer NN (with layers for input, hidden and output). The hidden layer has 5 perceptrons. \n",
    "2.\tA 3-layer NN. The hidden layer has 4 perceptrons.\n",
    "3.\tA 3-layer NN. The hidden layer has 3 perceptrons. \n",
    "4.\tA 5-layer NN. The 1st, 2nd, and 3rd hidden layers have 8, 4, and 8 perceptrons, respectively. \n",
    "5.\tRepeat 4) above, but this time we use the Rectified Linear Unit (ReLU), i.e., f(u) = max(0, u), for all perceptron activation. Compare the average running time of 5 runs of the two methods (correspondingly, each with the same initial random weights). (5 pts)\n",
    "\n",
    "What to report: Run your program 5 times with different initial weights and compare the stable states of all hidden layers from all these 5 runs on each autoencoder architecture. Report and compare the stable states of all hidden layers after you train the autoencoders using different initial weights. Discuss the correspondence of these states and the input values.  For problem 5) above, compare and discuss the stable states using the two different activation functions. If you cannot get stable states for one or both of these activation functions, explain the reason(s) why you cannot have stable states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Generating data and implementing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ones = np.ones(16)\n",
    "x = np.diag(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion **\n",
    "\n",
    "For this homework, I used sklearn Multi-layer Perceptron classifier. For autoencoder, the input and output should be the same. Each element in x represents one number. For example, we could treat the first line printed above as 1, second line as 2, and so on. The sklearn model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "\n",
    "In a typical run, I have the following codes.\n",
    "```python\n",
    "clf = MLPClassifier(activation = activation_function, learning_rate_init= 0.01, alpha=1e-5, hidden_layer_sizes= layers ,max_iter=200000, random_state=i)\n",
    "clf.fit(x,y)\n",
    "```\n",
    "\n",
    "parameters : \n",
    "\n",
    "* activation: activation function for the hidden layer. ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)); ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "* learning_rate_init: the initial learning rate used. It controls the step-size in updating the weights. \n",
    "* alpha: L2 penalty (regularization term) parameter.\n",
    "* hidden_layer_sizes:  tuple, length = n_layers - 2, default (100,)mThe ith element represents the number of neurons in the ith hidden layer.\n",
    "* max_iter: maximum number of iterations.\n",
    "* random_state: if int, random_state is the seed used by the random number generator. In this problem , I used 0,1,2,3,4 to seed the 5 intial random weights.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "* loss = clf.loss_  : loss computed with the loss function.\n",
    "* score = clf.score(x,y) : returns the mean accuracy on the given test data and labels. In this case, it represents the training accuracy.\n",
    "* weights = clf.coefs_ [0] : clf.coefs_ returns the ith element in the list represents the weight matrix corresponding to layer i. In 3-layer NN, clf.coefs_ [0] represents the weight from input layer to hidden layer.\n",
    "* bias = clf.intercepts_[0] : clf.intercepts_ returns the ith element in the list represents the bias vector corresponding to layer i + 1.\n",
    "\n",
    "\n",
    "Helper function:\n",
    "\n",
    "```python\n",
    "def compare(layers, activation_function, printHidden, learning_rate):\n",
    "```\n",
    "Compare function will compare the results of each NN with 5 different intial weights. It will gives loss and training accuracy of the model. Inside the function, I also called get_hiddenLayer_output() and convert_output_to01() functions to print out the hidden layer outputs(both original and converted) for all 16 numbers. I also checked if the binary code in hidden layer could represent all the numbers or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function helps to compare the results of NN with 5 different intial weights.\n",
    "def compare(layers, activation_function, printHidden, learning_rate):\n",
    "    #parameter:\n",
    "    # layers: tuple, a list of perceptron numbers in hidden layers\n",
    "    # activation_function: string, the activation function passed into the MLPClassifier\n",
    "    # printHidden, boolean, if true, print out the hidden layer outputs (original and converted)\n",
    "    # learning rate : the learning rate passed into the MLPClassifier\n",
    "    \n",
    "    # in each iteration, generate one set of seeded intial wieghts\n",
    "    for i in range(5):\n",
    "        clf = MLPClassifier(activation = activation_function, learning_rate_init= learning_rate, alpha=1e-5, hidden_layer_sizes= layers ,max_iter=200000, random_state=i)\n",
    "        clf.fit(x, y)\n",
    "        loss = clf.loss_\n",
    "        score = clf.score(x,y)\n",
    "        weights = clf.coefs_ [0]\n",
    "        bias = clf.intercepts_[0]\n",
    "        print \"random intialization \", i + 1, \": \", \"score is \", score, \" ; loss is \" , loss\n",
    "        \n",
    "        #print out the hidden layer outputs\n",
    "        if printHidden:\n",
    "            #original hidden layer outputs\n",
    "            print \"hidden layer output:\"\n",
    "            hidden_output = get_hiddenLayer_output(x, weights, bias)\n",
    "            #convert hidden layer outputs to 0/1 for discussion simplicity\n",
    "            print \"convert hidden layer output to 0/1:\"\n",
    "            coverted_hidden_output = convert_output_to01(hidden_output)\n",
    "            #print out the duplicate binary code in hidden layer\n",
    "            duplicates = check_binary(coverted_hidden_output)\n",
    "            if len(duplicates) == 0:\n",
    "                print \"16 numbers is successfully represented using different binary code in the hidden layer.\"\n",
    "            else:\n",
    "                print \"Failed to represent 16 numbers using different binary code in the hidden layer.\"\n",
    "                print \"Duplicates:\", duplicates\n",
    "                \n",
    "        #print out the final layer output \n",
    "        predicts = clf.predict(x)\n",
    "        print \"final output\"\n",
    "        print predicts\n",
    "        print \"Accuracy: \", clf.score(x, y)\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function computes the hidden layer outputs\n",
    "def get_hiddenLayer_output(x, weights, bias):\n",
    "    output = []\n",
    "    # loop through all the data points\n",
    "    for i in range(len(x)) :\n",
    "        total = []\n",
    "        # for each data point, compute its corresponding hidden layer outputs with trained weights and bias\n",
    "        for weight, b in zip(weights.T,bias.T): \n",
    "            total.append(sigmoid(np.dot(x[i], weight) + b))\n",
    "        # print out the original data and hidden layer outputs\n",
    "        print i , \": \", total\n",
    "        output.append(total)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function conver the hidden layer outputs into 0/1 for discussion simplicity\n",
    "def convert_output_to01(output):\n",
    "    new_output = []\n",
    "    for i in range(len(output)):\n",
    "        new_o = []\n",
    "        for num in output[i]:\n",
    "            if num > 0.5:\n",
    "                new_o.append(1)\n",
    "            else:\n",
    "                new_o.append(0)\n",
    "        print i , \": \", new_o\n",
    "        new_output.append(new_o)\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function checks to see if the hidden layer achieves successful binary representatoin\n",
    "# return the duplicates \n",
    "def check_binary(output):\n",
    "    \n",
    "    #convert the list of 0/1 to a string to represent the number\n",
    "    new_output = [str(o) for o in output]  \n",
    "    from collections import Counter\n",
    "    duplicates = [k for k,v in Counter(new_output).items() if v>1]\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 3-layer NN with 5 perceptrons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random intialization  1 :  score is  1.0  ; loss is  0.137348771489\n",
      "hidden layer output:\n",
      "0 :  [0.9972535907544947, 0.000929066632688839, 0.9977038356245037, 0.15958338126167376, 0.9956898015004597]\n",
      "1 :  [0.9987519252402189, 0.9990010391742221, 0.9977091944714267, 0.8201641728789094, 0.0005687283904284266]\n",
      "2 :  [0.0011717553543020343, 0.997699061924621, 0.998616709870235, 0.6259694732717773, 0.8842482966261132]\n",
      "3 :  [0.9983763628461368, 0.0007353220129544848, 0.9972319092746398, 0.9962404732174293, 0.013468187891325636]\n",
      "4 :  [0.9956865278932195, 0.4191724794637822, 0.06178262698521692, 0.9983028393553528, 0.01242805136878371]\n",
      "5 :  [0.036170303981862106, 0.2760042584441363, 0.9989161459643173, 0.5868285133071445, 0.00565626637786696]\n",
      "6 :  [0.8468067376374383, 0.9979168643140712, 0.9218298508861726, 0.001084355913936453, 0.9994341517949658]\n",
      "7 :  [0.9933327294321476, 0.0007054880480198448, 0.15648473247266953, 0.9991401973896682, 0.9972383527212512]\n",
      "8 :  [0.9969383345652046, 0.5531132683739146, 0.9989437945788803, 0.0023736797518378316, 0.30831660774078845]\n",
      "9 :  [0.996172717837677, 0.36086626939786776, 0.1012390174985109, 0.13241791596575567, 0.9987977239956821]\n",
      "10 :  [0.07455314804993025, 0.9964189753499926, 0.39326087528409254, 0.9983199456894006, 0.1273177230754866]\n",
      "11 :  [0.0041015470551562854, 0.5466066075191598, 0.16704026202450883, 0.9981797761623751, 0.9950378662435959]\n",
      "12 :  [0.009614030903702327, 0.002830160203375176, 0.37011044582974323, 0.9989015198782236, 0.13870261372840534]\n",
      "13 :  [0.9795417640244012, 0.9973401568494161, 0.2167835162119778, 0.14166583890793574, 0.35755600470358556]\n",
      "14 :  [0.006596130262796511, 0.004657244933767944, 0.9981570833533466, 0.99297467510933, 0.9991784342772468]\n",
      "15 :  [0.9971283501617157, 0.9987217943602996, 0.0007137663463743101, 0.9989532631652419, 0.9978140122768184]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 1, 0, 1]\n",
      "1 :  [1, 1, 1, 1, 0]\n",
      "2 :  [0, 1, 1, 1, 1]\n",
      "3 :  [1, 0, 1, 1, 0]\n",
      "4 :  [1, 0, 0, 1, 0]\n",
      "5 :  [0, 0, 1, 1, 0]\n",
      "6 :  [1, 1, 1, 0, 1]\n",
      "7 :  [1, 0, 0, 1, 1]\n",
      "8 :  [1, 1, 1, 0, 0]\n",
      "9 :  [1, 0, 0, 0, 1]\n",
      "10 :  [0, 1, 0, 1, 0]\n",
      "11 :  [0, 1, 0, 1, 1]\n",
      "12 :  [0, 0, 0, 1, 0]\n",
      "13 :  [1, 1, 0, 0, 0]\n",
      "14 :  [0, 0, 1, 1, 1]\n",
      "15 :  [1, 1, 0, 1, 1]\n",
      "16 numbers is successfully represented using different binary code in the hidden layer.\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  2 :  score is  1.0  ; loss is  0.136616868856\n",
      "hidden layer output:\n",
      "0 :  [0.9982094023522414, 0.9446621154146996, 0.9888162661451103, 0.46572387758285366, 0.0014684705392965893]\n",
      "1 :  [0.09274404303483455, 0.5277817356229039, 0.9988933537418324, 0.9987674999118871, 0.0012085329163944588]\n",
      "2 :  [0.9973467754633759, 0.002874551484756977, 0.010290049612884098, 0.15534833025207537, 0.996943440531294]\n",
      "3 :  [0.003787906292623995, 0.996966047525948, 0.0014571173297968658, 0.9986122629559371, 0.9987533876578949]\n",
      "4 :  [0.9979719879551966, 0.8842921427750633, 0.006473835307630632, 0.9982594364118728, 0.0810485935926812]\n",
      "5 :  [0.8598957972546547, 0.002543553286505078, 0.003527680082221499, 0.9988225397609272, 0.9931627136711434]\n",
      "6 :  [0.0506550657629803, 0.11359482009096479, 0.006402512112297656, 0.1629271476598385, 0.9973321096570923]\n",
      "7 :  [0.0021122455870608184, 0.9556648913119457, 0.9963320550971585, 0.26317644106711746, 0.12521137850274455]\n",
      "8 :  [0.9992166958324377, 0.9988476010145078, 0.99879003450804, 0.0007016434101177385, 0.9968408768130463]\n",
      "9 :  [0.998337427045215, 0.0007062732340414528, 0.9980759008173596, 0.10879324304485734, 0.81953284312907]\n",
      "10 :  [0.0016853013849421983, 0.005616367557254482, 0.995935216109521, 0.0738949556990744, 0.9966296217339342]\n",
      "11 :  [0.9899736481388242, 0.9982508543527248, 0.0008389070236250177, 0.18699864630721386, 0.8767861644110662]\n",
      "12 :  [0.0010738216753254713, 0.9983836063419426, 0.5975061181843315, 0.07132503041801239, 0.9988259772565533]\n",
      "13 :  [0.0015302245259598893, 0.0020300221831419606, 0.9604899730606444, 0.999224839925745, 0.9966127534723159]\n",
      "14 :  [0.9981366966886102, 0.001026693839507419, 0.9718264508367808, 0.9966229816178994, 0.23912779663133582]\n",
      "15 :  [0.0020887945731280343, 0.9981493269583955, 0.2872036326538844, 0.9709208221135636, 0.0298003801477197]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 1, 1, 0, 0]\n",
      "1 :  [0, 1, 1, 1, 0]\n",
      "2 :  [1, 0, 0, 0, 1]\n",
      "3 :  [0, 1, 0, 1, 1]\n",
      "4 :  [1, 1, 0, 1, 0]\n",
      "5 :  [1, 0, 0, 1, 1]\n",
      "6 :  [0, 0, 0, 0, 1]\n",
      "7 :  [0, 1, 1, 0, 0]\n",
      "8 :  [1, 1, 1, 0, 1]\n",
      "9 :  [1, 0, 1, 0, 1]\n",
      "10 :  [0, 0, 1, 0, 1]\n",
      "11 :  [1, 1, 0, 0, 1]\n",
      "12 :  [0, 1, 1, 0, 1]\n",
      "13 :  [0, 0, 1, 1, 1]\n",
      "14 :  [1, 0, 1, 1, 0]\n",
      "15 :  [0, 1, 0, 1, 0]\n",
      "16 numbers is successfully represented using different binary code in the hidden layer.\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  3 :  score is  1.0  ; loss is  0.140909145541\n",
      "hidden layer output:\n",
      "0 :  [0.9986443742139589, 0.001282090810727334, 0.15839257760684197, 0.1866924333935128, 0.9372539534413635]\n",
      "1 :  [0.0008107833265604302, 0.998417539996992, 0.9817991933668102, 0.9964063676525752, 0.21666193269842465]\n",
      "2 :  [0.9987238191568933, 0.9989563099507242, 0.9977593223959509, 0.9979870065231699, 0.0006689941988699406]\n",
      "3 :  [0.9960779275714067, 0.0009355273793604011, 0.9981976863610701, 0.9986511597842239, 0.0028117571199998454]\n",
      "4 :  [0.001997531770053088, 0.08300187043987131, 0.9959027560689915, 0.9955186417004726, 0.16667845446577526]\n",
      "5 :  [0.9977804584414691, 0.004098914410611449, 0.9901776735471486, 0.07430660227816273, 0.3008277082120215]\n",
      "6 :  [0.0021898838877525317, 0.9967286038120373, 0.9932580317230252, 0.12250189540830814, 0.9973656005474236]\n",
      "7 :  [0.8304453252309008, 0.9981172795161701, 0.0023578771434902518, 0.997568006611841, 0.3031484748987509]\n",
      "8 :  [0.6045043359267896, 0.08472922545119847, 0.9988148567247298, 0.0011227185206116835, 0.9989145687890557]\n",
      "9 :  [0.0016496965471712304, 0.0011842111175285653, 0.9950772926891078, 0.7967898855297579, 0.9990362491432054]\n",
      "10 :  [0.29025918885004287, 0.5325401665424662, 0.998445198480739, 0.03286959205169815, 0.06893169997120113]\n",
      "11 :  [0.4046257416585652, 0.0038628850581099695, 0.0038814466181316936, 0.9990731626729428, 0.9976632440211634]\n",
      "12 :  [0.9978257328084743, 0.039481529241572985, 0.008563116407799146, 0.9923500418627619, 0.27862844501764233]\n",
      "13 :  [0.0019810698480577384, 0.9926958034116276, 0.22417345808053404, 0.9983469854787409, 0.9959820367602442]\n",
      "14 :  [0.9978535475942983, 0.9919441040998288, 0.4241597303039476, 0.0011329184518574238, 0.3316588871611131]\n",
      "15 :  [0.7642182836592024, 0.996134904641171, 0.001358465209412759, 0.33018909964922366, 0.9990128508009618]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 0, 0, 1]\n",
      "1 :  [0, 1, 1, 1, 0]\n",
      "2 :  [1, 1, 1, 1, 0]\n",
      "3 :  [1, 0, 1, 1, 0]\n",
      "4 :  [0, 0, 1, 1, 0]\n",
      "5 :  [1, 0, 1, 0, 0]\n",
      "6 :  [0, 1, 1, 0, 1]\n",
      "7 :  [1, 1, 0, 1, 0]\n",
      "8 :  [1, 0, 1, 0, 1]\n",
      "9 :  [0, 0, 1, 1, 1]\n",
      "10 :  [0, 1, 1, 0, 0]\n",
      "11 :  [0, 0, 0, 1, 1]\n",
      "12 :  [1, 0, 0, 1, 0]\n",
      "13 :  [0, 1, 0, 1, 1]\n",
      "14 :  [1, 1, 0, 0, 0]\n",
      "15 :  [1, 1, 0, 0, 1]\n",
      "16 numbers is successfully represented using different binary code in the hidden layer.\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  4 :  score is  1.0  ; loss is  0.150078818001\n",
      "hidden layer output:\n",
      "0 :  [0.9939375341588217, 0.9970630181606195, 0.09995469927397022, 0.9991799075191976, 0.0009039468997227457]\n",
      "1 :  [0.9978677166939856, 0.9968693652996101, 0.18532154406660087, 0.06381940356564497, 0.0031253080822390934]\n",
      "2 :  [0.0004216706897164205, 0.9983099940671692, 0.6787365806634706, 0.9991239783905217, 0.998439408489361]\n",
      "3 :  [0.995608930819414, 0.9952472821510682, 0.033794071416128446, 0.21934584325989048, 0.9981511988000672]\n",
      "4 :  [0.08023289911742518, 0.02627103702963796, 0.9983684984453218, 0.9966615895840414, 0.002205200763028693]\n",
      "5 :  [0.6676670480730712, 0.01218476716649854, 0.9981547334584466, 0.16818372451781433, 0.9922135229823282]\n",
      "6 :  [0.999046966424272, 0.9992989928564392, 0.998729205441015, 0.00048163428606719976, 0.9985880423512785]\n",
      "7 :  [0.5194472758829357, 0.9993015941477222, 0.0016707867245600314, 0.9988877045624777, 0.9660101885898823]\n",
      "8 :  [0.32571851681611247, 0.9989316714413645, 0.9983153704949481, 0.05493321037969245, 0.0010690779069145785]\n",
      "9 :  [0.9993606938921857, 0.023740769292481225, 0.9913524714285045, 0.9977320998216059, 0.0019384280704832456]\n",
      "10 :  [0.9969654936663892, 0.3794920174402643, 0.9988479308791159, 0.07829427087069905, 0.0032060955186865277]\n",
      "11 :  [0.9991264325631336, 0.5027748548048365, 0.0011210091412578508, 0.9972172633926057, 0.8487398024278741]\n",
      "12 :  [0.011783869872077359, 0.039470177107124894, 0.9985201997985081, 0.9978004263559493, 0.9961191517432303]\n",
      "13 :  [0.9986199105470029, 0.0004747381302886675, 0.6609057904961364, 0.9991499863495764, 0.9980301917884792]\n",
      "14 :  [0.013506002325121393, 0.7558558611934567, 0.9985368372891845, 0.1810482694252365, 0.9896989210984501]\n",
      "15 :  [0.0034611603716027273, 0.9986703293370153, 0.9905069348714074, 0.9988934387792853, 0.0014753792844354207]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 1, 0, 1, 0]\n",
      "1 :  [1, 1, 0, 0, 0]\n",
      "2 :  [0, 1, 1, 1, 1]\n",
      "3 :  [1, 1, 0, 0, 1]\n",
      "4 :  [0, 0, 1, 1, 0]\n",
      "5 :  [1, 0, 1, 0, 1]\n",
      "6 :  [1, 1, 1, 0, 1]\n",
      "7 :  [1, 1, 0, 1, 1]\n",
      "8 :  [0, 1, 1, 0, 0]\n",
      "9 :  [1, 0, 1, 1, 0]\n",
      "10 :  [1, 0, 1, 0, 0]\n",
      "11 :  [1, 1, 0, 1, 1]\n",
      "12 :  [0, 0, 1, 1, 1]\n",
      "13 :  [1, 0, 1, 1, 1]\n",
      "14 :  [0, 1, 1, 0, 1]\n",
      "15 :  [0, 1, 1, 1, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 1, 0, 1, 1]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  5 :  score is  1.0  ; loss is  0.125280615329\n",
      "hidden layer output:\n",
      "0 :  [0.8400902317050262, 0.049157527997660884, 0.9924519267330002, 0.06960125285696174, 0.8161832424416874]\n",
      "1 :  [0.5045142773694302, 0.5619285420312474, 0.031838846097778595, 0.9957605777214912, 0.07139343161282717]\n",
      "2 :  [0.998211960300413, 0.9985877979265446, 0.9990749743434271, 0.9991367239237392, 0.0003583957242887694]\n",
      "3 :  [0.4272002339803156, 0.5421887923839076, 0.11429517743391375, 0.08294857497223788, 0.9886342617209182]\n",
      "4 :  [0.9987125489522698, 0.998897182173482, 0.999223566116352, 0.0006009899496298656, 0.9986702486898693]\n",
      "5 :  [0.9880242161257747, 0.08627716452289251, 0.9948429917620768, 0.9888937848032531, 0.24481730367571503]\n",
      "6 :  [0.2511726242994306, 0.9956003079589613, 0.7294785456074825, 0.00255418705687679, 0.15406309902943882]\n",
      "7 :  [0.0004966753690043028, 0.9959197379579704, 0.9948684124670243, 0.24017981264347624, 0.9992559670180283]\n",
      "8 :  [0.9988428508962655, 0.9986476079971958, 0.00032397215331696975, 0.9888977176148943, 0.9986481658515795]\n",
      "9 :  [0.2500173126939024, 0.23991838434840537, 0.0016548715563639213, 0.996702021933982, 0.9957406739985225]\n",
      "10 :  [0.5356580043709819, 0.00035742741917121185, 0.999023789946749, 0.9972496654393739, 0.9979846776096011]\n",
      "11 :  [0.005486428336094804, 0.9989291641430763, 0.12396399814522736, 0.9975954960387453, 0.8430650227228861]\n",
      "12 :  [0.99941428708738, 0.0005728699531312052, 0.33461684511468875, 0.9981306648914843, 0.99663193129702]\n",
      "13 :  [0.996013405319616, 0.529044622328136, 0.4215456218502829, 0.09258375737149122, 0.13259998375933485]\n",
      "14 :  [0.013849468241665478, 0.9970486034550575, 0.9664223193623935, 0.9961777904021644, 0.19955444183523305]\n",
      "15 :  [0.0003747707018372607, 0.5215182465688977, 0.993691432991076, 0.9992976064203369, 0.9974497731389786]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 1, 0, 1]\n",
      "1 :  [1, 1, 0, 1, 0]\n",
      "2 :  [1, 1, 1, 1, 0]\n",
      "3 :  [0, 1, 0, 0, 1]\n",
      "4 :  [1, 1, 1, 0, 1]\n",
      "5 :  [1, 0, 1, 1, 0]\n",
      "6 :  [0, 1, 1, 0, 0]\n",
      "7 :  [0, 1, 1, 0, 1]\n",
      "8 :  [1, 1, 0, 1, 1]\n",
      "9 :  [0, 0, 0, 1, 1]\n",
      "10 :  [1, 0, 1, 1, 1]\n",
      "11 :  [0, 1, 0, 1, 1]\n",
      "12 :  [1, 0, 0, 1, 1]\n",
      "13 :  [1, 1, 0, 0, 0]\n",
      "14 :  [0, 1, 1, 1, 0]\n",
      "15 :  [0, 1, 1, 1, 1]\n",
      "16 numbers is successfully represented using different binary code in the hidden layer.\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare((5,) , 'logistic', True, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion **\n",
    "\n",
    "With learning rate set as 0.01, all 5 runs for NN(5 perceptrons in hidden layer) converges and gives very good training accuracy score (1.0) and very low loss( < 0.16). All five runs generated the final output exactly same as the input with accuracy as 1.0. The final output is stable.\n",
    "\n",
    "The original hidden layer outputs are a set of real numbers in the range of 0 to 1 (see above tables). For each number, we got 5 corresponding stable states of hidden layer nodes, as shown in above section. For example, in first run, we got [0.9972535907544947, 0.000929066632688839, 0.9977038356245037, 0.15958338126167376, 0.9956898015004597] represents 0. These real numbers are close to either 1 or 0 with few exceptions. \n",
    "\n",
    "\n",
    "For comparing simplicity, I set a threshold of 0.5 to convert these real numbers into 0 or 1, i.e., any number bigger than 0.5 set as 1 and 0 otherwise. This would gives us the binary represenation of the input number in the hidden layer.\n",
    "\n",
    "Now we could clearly see that inside each run except run 4, we achieves successfully binary representation of each number in the hidden layer, i.e., we could use different binary representation for different number. This makes sense since we only need 4 perceptrons to binarily represent 16 numbers. The only exception is run 4, where we use [1, 1, 0, 1, 1] represented both 8 and 12. The algorithm still converges and gives 100% accuracy possibly due the fact that in practice we used real number rather than binary number in hidden layer.\n",
    "\n",
    "Between different runs, since we start with different intial weights, the final binary representation for a specific number may change. For example, we have [1, 0, 1, 0, 1] represents 0 in the first run and [1, 1, 1, 0, 0] represents 0 in the second run. Although both runs generated different binary representation for 0, they both successfullly achieved binary representation for 16 numbers in their own way. That is to say the sets of binary codes are the same among the runs. In this sense, I think the states of the hidden layer are stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 3-layer NN with 4 perceptrons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random intialization  1 :  score is  0.875  ; loss is  0.314742439679\n",
      "hidden layer output:\n",
      "0 :  [0.9985758138142072, 0.011365529725903415, 0.03986725213658222, 0.7020097843469328]\n",
      "1 :  [0.08455481732657076, 0.85011406460693, 0.0013804303546094713, 0.46188754098595547]\n",
      "2 :  [0.999490806778133, 0.0017059006960237, 0.9990359542107083, 0.9989584618046012]\n",
      "3 :  [0.2701122656350515, 0.00029050775210826457, 0.9976241215451325, 0.5979422579406634]\n",
      "4 :  [0.9958531479712722, 0.9978342076711461, 0.0006451087353640342, 0.9934537782458278]\n",
      "5 :  [0.9770562097294104, 0.9982123599025229, 0.0010444136789722412, 0.04127654937409666]\n",
      "6 :  [0.11307209592513297, 0.12160701761247149, 0.07048160352213113, 0.8175773974917608]\n",
      "7 :  [0.00045897456416944316, 0.43540810713780326, 0.9978807033049315, 0.9981189937990026]\n",
      "8 :  [0.9987070022985127, 0.9976430373365605, 0.00027203442810131687, 0.9985987607108713]\n",
      "9 :  [0.0010489176489690145, 0.9981452605844843, 0.9994193548130402, 0.3318186297255382]\n",
      "10 :  [0.9990778283210064, 0.9988213397703491, 0.9991138682603681, 0.0002143469398646829]\n",
      "11 :  [0.1702727691004017, 0.9964718824821779, 0.35790929043621544, 0.0004967622363265749]\n",
      "12 :  [0.014207623980484824, 0.1494491971786631, 0.4670219477315059, 0.0018756490755562861]\n",
      "13 :  [0.9574040596229658, 0.14142487139549342, 0.17426160283492925, 0.00127056085664441]\n",
      "14 :  [0.0026584159086232464, 0.9993266681100785, 0.5233094180690926, 0.9994027120007766]\n",
      "15 :  [0.9266245281190718, 0.0036856394564075733, 0.9949686109191129, 0.04879795660260469]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 0, 1]\n",
      "1 :  [0, 1, 0, 0]\n",
      "2 :  [1, 0, 1, 1]\n",
      "3 :  [0, 0, 1, 1]\n",
      "4 :  [1, 1, 0, 1]\n",
      "5 :  [1, 1, 0, 0]\n",
      "6 :  [0, 0, 0, 1]\n",
      "7 :  [0, 0, 1, 1]\n",
      "8 :  [1, 1, 0, 1]\n",
      "9 :  [0, 1, 1, 0]\n",
      "10 :  [1, 1, 1, 0]\n",
      "11 :  [0, 1, 0, 0]\n",
      "12 :  [0, 0, 0, 0]\n",
      "13 :  [1, 0, 0, 0]\n",
      "14 :  [0, 1, 1, 1]\n",
      "15 :  [1, 0, 1, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[0, 1, 0, 0]', '[0, 0, 1, 1]', '[1, 1, 0, 1]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  0.875\n",
      "\n",
      "random intialization  2 :  score is  0.875  ; loss is  0.306150677373\n",
      "hidden layer output:\n",
      "0 :  [0.9969022278154223, 0.30031722862949933, 0.0010131445061037818, 0.21801005401205428]\n",
      "1 :  [0.2590825475961345, 0.9987250862711322, 0.0003711825985387279, 0.4870669725549476]\n",
      "2 :  [0.18628828908130957, 0.07311205731574898, 0.9354304287261014, 0.09365607899717387]\n",
      "3 :  [0.000345440013955604, 0.9979971207201361, 0.4422050208459134, 0.9992444002948226]\n",
      "4 :  [0.015024155002070999, 0.4612130223149359, 0.2398758724464885, 0.00023917851350777568]\n",
      "5 :  [0.9987133200409394, 0.9983316456126397, 0.9969418057910002, 0.000568627143350801]\n",
      "6 :  [0.8208677950239209, 0.9988210059412136, 0.10928146214383745, 0.009300903926300839]\n",
      "7 :  [0.0007440165870249626, 0.999168877678353, 0.9990390001129734, 0.3689803929353241]\n",
      "8 :  [0.9981381758071841, 0.0006294001383699954, 0.9996573048606434, 0.9979347133400127]\n",
      "9 :  [0.004153812197377419, 0.47379250910679305, 0.9994701105059056, 0.9991152630945288]\n",
      "10 :  [0.9995614609005073, 0.999373215566094, 0.0008044954242704865, 0.9984638471184548]\n",
      "11 :  [0.36141374052155345, 0.3626380903768987, 0.000211382899230098, 0.9981322379365208]\n",
      "12 :  [0.9985203461109781, 0.9972135789548087, 0.9975844642645991, 0.0010477242639634111]\n",
      "13 :  [0.9979351204719126, 0.022001760893799596, 0.6663561457577611, 0.02185233342072166]\n",
      "14 :  [0.9988586607658099, 0.0006208330286838956, 0.24453993870602286, 0.9807000912470778]\n",
      "15 :  [0.0429755201622761, 0.0007531679168519484, 0.3846265312149927, 0.6999302124533856]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 0, 0]\n",
      "1 :  [0, 1, 0, 0]\n",
      "2 :  [0, 0, 1, 0]\n",
      "3 :  [0, 1, 0, 1]\n",
      "4 :  [0, 0, 0, 0]\n",
      "5 :  [1, 1, 1, 0]\n",
      "6 :  [1, 1, 0, 0]\n",
      "7 :  [0, 1, 1, 0]\n",
      "8 :  [1, 0, 1, 1]\n",
      "9 :  [0, 0, 1, 1]\n",
      "10 :  [1, 1, 0, 1]\n",
      "11 :  [0, 0, 0, 1]\n",
      "12 :  [1, 1, 1, 0]\n",
      "13 :  [1, 0, 1, 0]\n",
      "14 :  [1, 0, 0, 1]\n",
      "15 :  [0, 0, 0, 1]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 1, 1, 0]', '[0, 0, 0, 1]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  0.875\n",
      "\n",
      "random intialization  3 :  score is  1.0  ; loss is  0.153613627004\n",
      "hidden layer output:\n",
      "0 :  [0.9989046753597879, 0.0010918755860748952, 0.9980651176046654, 0.9993870177166921]\n",
      "1 :  [0.9992016594995028, 0.20168478712274457, 0.0003030605158348931, 0.9979495028197408]\n",
      "2 :  [0.14603259176622646, 0.0003899601419885088, 0.998148752598798, 0.5575454492497667]\n",
      "3 :  [0.008288863364821005, 0.9989282402121454, 0.9990954942114515, 0.999266583755323]\n",
      "4 :  [0.00021385093360401993, 0.9986797394880083, 0.36728070889163233, 0.37234177049645856]\n",
      "5 :  [0.22476886621774533, 0.000537816720567742, 0.00034978132801199605, 0.5332904846644752]\n",
      "6 :  [0.9984551411291408, 0.9996523861893155, 0.0002508636168269972, 0.9787223180389476]\n",
      "7 :  [0.3016234745101355, 0.2029800830279835, 0.813066750160761, 0.0010955717804571083]\n",
      "8 :  [0.9994836973197967, 0.9946553366090893, 0.23223198674515222, 0.0008118640236366274]\n",
      "9 :  [0.18994104670598871, 0.773862113558946, 0.0008813775092439915, 0.9950044993188595]\n",
      "10 :  [0.6683399176023164, 0.15152258910383612, 0.00082581192832828, 0.07573758291195484]\n",
      "11 :  [0.0005361804703772324, 0.23727024540589056, 0.4778416995692862, 0.9988086057280045]\n",
      "12 :  [0.38868213918299904, 0.8954589787637117, 0.0007374871271078408, 0.07468561376530113]\n",
      "13 :  [0.26209418307367466, 0.9979451867395619, 0.8751852918280884, 0.0009668244059071177]\n",
      "14 :  [0.9991397479644365, 0.00047593211720165255, 0.6972382793607416, 0.19250536922552036]\n",
      "15 :  [0.9994793051077485, 0.9950805655890129, 0.9994811136067312, 0.0009254744330604086]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 1, 1]\n",
      "1 :  [1, 0, 0, 1]\n",
      "2 :  [0, 0, 1, 1]\n",
      "3 :  [0, 1, 1, 1]\n",
      "4 :  [0, 1, 0, 0]\n",
      "5 :  [0, 0, 0, 1]\n",
      "6 :  [1, 1, 0, 1]\n",
      "7 :  [0, 0, 1, 0]\n",
      "8 :  [1, 1, 0, 0]\n",
      "9 :  [0, 1, 0, 1]\n",
      "10 :  [1, 0, 0, 0]\n",
      "11 :  [0, 0, 0, 1]\n",
      "12 :  [0, 1, 0, 0]\n",
      "13 :  [0, 1, 1, 0]\n",
      "14 :  [1, 0, 1, 0]\n",
      "15 :  [1, 1, 1, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[0, 1, 0, 0]', '[0, 0, 0, 1]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  4 :  score is  0.875  ; loss is  0.306747430522\n",
      "hidden layer output:\n",
      "0 :  [0.9958529719153251, 0.29407906430434255, 0.001333424143519514, 0.9968516933954508]\n",
      "1 :  [0.9994348896823277, 0.9993725072948236, 0.9910766755523622, 0.0009341302316222348]\n",
      "2 :  [0.9969849056563643, 0.2402273810889943, 0.9973738833859024, 0.0005091347854435559]\n",
      "3 :  [0.3875741557661233, 0.0002689390273879463, 0.9988475276869037, 0.2602007385269253]\n",
      "4 :  [0.2004662670949119, 0.9995432933201377, 0.9984656996081224, 0.0030077765869529925]\n",
      "5 :  [0.003058190281853307, 0.9268359935397689, 0.10328717283656277, 0.3149625048095176]\n",
      "6 :  [0.43090469813937415, 0.9992362712110715, 0.0021864444254098667, 0.9994956043210821]\n",
      "7 :  [0.0016323688290004023, 0.21225330552893668, 0.9991153230737295, 0.8675689454874361]\n",
      "8 :  [0.00046002959210858366, 0.33831559142321743, 0.7683660767187065, 0.0014590764872303866]\n",
      "9 :  [0.0016132614483621922, 0.9919462857088617, 0.9978941757298255, 0.9933383906598791]\n",
      "10 :  [0.0031193670517223413, 0.08168487160072989, 0.04442060162682022, 0.5766699622149196]\n",
      "11 :  [0.0003263598429122492, 0.9988645103307574, 0.997190623270861, 0.9984972312789143]\n",
      "12 :  [0.3568817514070429, 0.35727323647137355, 0.09955609313921204, 0.0002906081521436462]\n",
      "13 :  [0.9967078912197384, 0.0006667023147031091, 0.28904639617054595, 0.3182533306653343]\n",
      "14 :  [0.9983862369941435, 0.9977492966526721, 0.0004460644602202433, 0.5120674020486622]\n",
      "15 :  [0.9970523583366333, 0.00040582749394763344, 0.9981871363405852, 0.9992858700210281]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 0, 1]\n",
      "1 :  [1, 1, 1, 0]\n",
      "2 :  [1, 0, 1, 0]\n",
      "3 :  [0, 0, 1, 0]\n",
      "4 :  [0, 1, 1, 0]\n",
      "5 :  [0, 1, 0, 0]\n",
      "6 :  [0, 1, 0, 1]\n",
      "7 :  [0, 0, 1, 1]\n",
      "8 :  [0, 0, 1, 0]\n",
      "9 :  [0, 1, 1, 1]\n",
      "10 :  [0, 0, 0, 1]\n",
      "11 :  [0, 1, 1, 1]\n",
      "12 :  [0, 0, 0, 0]\n",
      "13 :  [1, 0, 0, 0]\n",
      "14 :  [1, 1, 0, 1]\n",
      "15 :  [1, 0, 1, 1]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[0, 0, 1, 0]', '[0, 1, 1, 1]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  0.875\n",
      "\n",
      "random intialization  5 :  score is  0.875  ; loss is  0.320192527497\n",
      "hidden layer output:\n",
      "0 :  [0.29834167426400626, 0.9993187685945637, 0.045278505331470056, 0.1494356804759434]\n",
      "1 :  [0.00043943941710426727, 0.9988380674155172, 0.999644377838193, 0.9985255176289975]\n",
      "2 :  [0.9776221702598604, 0.5420990023654977, 0.10161403222990094, 0.0004738536862752422]\n",
      "3 :  [0.9981914334520373, 0.0005621075200203219, 0.10604983074517609, 0.37801151544954026]\n",
      "4 :  [0.042623738603910534, 0.07460378188707253, 0.9991968800518658, 0.405377148569829]\n",
      "5 :  [0.002531372397890094, 0.9980383851258636, 0.08475746760112984, 0.9969901860792885]\n",
      "6 :  [0.056544133552010996, 0.2088303293470273, 0.00036291257308709976, 0.3653902703128996]\n",
      "7 :  [0.9973801612766962, 0.0006234310767492695, 0.9982440739838119, 0.9982652683590806]\n",
      "8 :  [0.0012388278419330889, 0.08683551920534814, 0.49371158712618696, 0.9948849465001203]\n",
      "9 :  [0.16671725781128335, 0.9987716448580215, 0.9973066674570378, 0.009553004129567647]\n",
      "10 :  [0.99957176806822, 0.9989354226180939, 0.9983321555936256, 0.0008230143899779833]\n",
      "11 :  [0.6486229413474247, 0.15578790385336236, 0.0013960528998552802, 0.9994863743781535]\n",
      "12 :  [0.00038444571428182365, 0.2665529862355008, 0.43597254701002, 0.0006990600692148859]\n",
      "13 :  [0.9992328665528991, 0.9991413902222884, 0.0005590502723798599, 0.9989324578945986]\n",
      "14 :  [0.8015920320991352, 0.07378405848645543, 0.9971907638293898, 0.008726874516627545]\n",
      "15 :  [0.997421447241492, 0.0009327018137014656, 0.996368099101326, 0.9979128746173443]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [0, 1, 0, 0]\n",
      "1 :  [0, 1, 1, 1]\n",
      "2 :  [1, 1, 0, 0]\n",
      "3 :  [1, 0, 0, 0]\n",
      "4 :  [0, 0, 1, 0]\n",
      "5 :  [0, 1, 0, 1]\n",
      "6 :  [0, 0, 0, 0]\n",
      "7 :  [1, 0, 1, 1]\n",
      "8 :  [0, 0, 0, 1]\n",
      "9 :  [0, 1, 1, 0]\n",
      "10 :  [1, 1, 1, 0]\n",
      "11 :  [1, 0, 0, 1]\n",
      "12 :  [0, 0, 0, 0]\n",
      "13 :  [1, 1, 0, 1]\n",
      "14 :  [1, 0, 1, 0]\n",
      "15 :  [1, 0, 1, 1]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 1, 1]', '[0, 0, 0, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare((4,) , 'logistic', True, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With learning rate set as 0.01, all 5 runs for NN(4 perceptrons in hidden layer) converges and gives very good training accuracy and very low loss, though the result is not as good as the NN with 5 perceptrons in hidden layer. The final output is almost same as the input, with accuracy being 1.0(1 run) or 0.875(4 runs).The stable states of final output is different possibly due to the fact that with different random intialization the algorithm converged to different local minimum.\n",
    "\n",
    "The original hidden layer outputs are a set of real numbers in the range of 0 to 1. For each number, I calculated 4 corresponding states of hidden layer nodes after the algorithm converges, as shown in above section. For example, in run 1, I got [0.9985758138142072, 0.011365529725903415, 0.03986725213658222, 0.7020097843469328] to represent 0 in the hidden layer. We could see these real numbers are close to 1 or 0 with some exceptions.\n",
    "\n",
    "For comparing simplicity, we set a threshold of 0.5 to convert these real numbers into 0 or 1, i.e., any number bigger than 0.5 set as 1 and 0 otherwise. This would gives us the binary represenation of the input number in the hidden layer.\n",
    "\n",
    "Now we could clearly see that inside each run, we achieves some binary representation of each number in the hidden layer. However, if we just used binary representation, the hidden layer fails to represent all 16 numbers, because some binary code is used to represent mulitple numbers. For example, in run 1, [0, 1, 0, 0] represented both 2 and 12. This happens to all 5 runs. Interestingly, we should be able to use 4 perceptrons to represent 16 numbers in theory.  The algorithm still converges possibly due the fact that in practice we used real number rather than binary number in hidden layer. \n",
    "\n",
    "Between different runs, since we start with different intial weights, the final binary representation in hidden layer for a specific number may change. For example, we have [1, 0, 0, 1] represents 0 in the first run and [1, 0, 0, 0] represents 0 in the second run. And it is clearly shown that lot of difference exists between runs regarding the binary representation in hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 3-layer NN with 3 perceptrons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random intialization  1 :  score is  1.0  ; loss is  0.200107669423\n",
      "hidden layer output:\n",
      "0 :  [0.9997714609328417, 0.34513298724656155, 0.9997484394145042]\n",
      "1 :  [0.9997489483045012, 0.17966474328738452, 0.049653542714081796]\n",
      "2 :  [0.16349512647924785, 0.854989626884116, 0.82166899583689]\n",
      "3 :  [8.636079573402618e-05, 0.6282484326952336, 0.34768061096276454]\n",
      "4 :  [0.1597722433715804, 0.29486194281004835, 0.00016768273997130846]\n",
      "5 :  [0.30625254640311766, 0.9998495213206119, 0.3727902594280726]\n",
      "6 :  [0.7036247584680626, 0.9995409997189478, 0.9998505338021197]\n",
      "7 :  [0.5020640702559874, 0.7780276731349842, 8.991892803906307e-05]\n",
      "8 :  [0.44931625454394847, 0.00033051711173067626, 0.8567304013217348]\n",
      "9 :  [0.9998507648620385, 0.9998588614181981, 0.4115150971486991]\n",
      "10 :  [0.00020911638900628172, 0.23420055079781932, 0.5739178880185117]\n",
      "11 :  [0.5592694704896163, 0.027801391396730428, 0.0002211371002827763]\n",
      "12 :  [0.21440895234398963, 5.609858721299348e-05, 0.2877627045027906]\n",
      "13 :  [0.18189663166875789, 0.4142651576398111, 0.999865791954778]\n",
      "14 :  [0.9996331649193652, 0.6060386676918251, 0.0008699413138942705]\n",
      "15 :  [0.9996168673412026, 0.00012947428612865872, 0.5158644341145034]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 1]\n",
      "1 :  [1, 0, 0]\n",
      "2 :  [0, 1, 1]\n",
      "3 :  [0, 1, 0]\n",
      "4 :  [0, 0, 0]\n",
      "5 :  [0, 1, 0]\n",
      "6 :  [1, 1, 1]\n",
      "7 :  [1, 1, 0]\n",
      "8 :  [0, 0, 1]\n",
      "9 :  [1, 1, 0]\n",
      "10 :  [0, 0, 1]\n",
      "11 :  [1, 0, 0]\n",
      "12 :  [0, 0, 0]\n",
      "13 :  [0, 0, 1]\n",
      "14 :  [1, 1, 0]\n",
      "15 :  [1, 0, 1]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 0]', '[1, 0, 1]', '[0, 0, 1]', '[0, 0, 0]', '[1, 1, 0]', '[0, 1, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  2 :  score is  1.0  ; loss is  0.190450605695\n",
      "hidden layer output:\n",
      "0 :  [0.8172275178538656, 0.3307615974550232, 5.781100781226375e-05]\n",
      "1 :  [0.00028235997981780186, 0.8382018244464078, 0.16826970944885655]\n",
      "2 :  [6.482984581961517e-05, 0.23461527117866615, 0.5120250000475938]\n",
      "3 :  [0.12954058572166052, 0.3727855092729684, 4.223348305299131e-05]\n",
      "4 :  [0.27511500145971696, 8.007773808168567e-05, 0.6955466986352044]\n",
      "5 :  [0.9985533852935987, 0.0006551938563074936, 0.5377731475575702]\n",
      "6 :  [0.4485700659175406, 0.9997402635684115, 0.9998044500614615]\n",
      "7 :  [0.6631445742813411, 0.00013090210934099984, 0.1572887316744925]\n",
      "8 :  [0.999852940272836, 0.5826814835531949, 0.9997368397871983]\n",
      "9 :  [0.16379206548647676, 0.45376270568938293, 0.9997133125691714]\n",
      "10 :  [0.6401325778911373, 0.147120449675552, 0.999767440601544]\n",
      "11 :  [0.9966817300279652, 0.9998701313750206, 0.5979610229620651]\n",
      "12 :  [0.9998306404137705, 0.7544036165154747, 0.1513668578131175]\n",
      "13 :  [0.0003235260628857835, 0.9984891096837233, 0.6169392091707101]\n",
      "14 :  [0.21975048932154886, 0.0001460279363862074, 0.23423220691900143]\n",
      "15 :  [0.4627242200104199, 0.9994832096071656, 0.0021618692281134035]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [1, 0, 0]\n",
      "1 :  [0, 1, 0]\n",
      "2 :  [0, 0, 1]\n",
      "3 :  [0, 0, 0]\n",
      "4 :  [0, 0, 1]\n",
      "5 :  [1, 0, 1]\n",
      "6 :  [0, 1, 1]\n",
      "7 :  [1, 0, 0]\n",
      "8 :  [1, 1, 1]\n",
      "9 :  [0, 0, 1]\n",
      "10 :  [1, 0, 1]\n",
      "11 :  [1, 1, 1]\n",
      "12 :  [1, 1, 0]\n",
      "13 :  [0, 1, 1]\n",
      "14 :  [0, 0, 0]\n",
      "15 :  [0, 1, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 0]', '[1, 0, 1]', '[1, 1, 1]', '[0, 0, 1]', '[0, 0, 0]', '[0, 1, 1]', '[0, 1, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  3 :  score is  1.0  ; loss is  0.204010754479\n",
      "hidden layer output:\n",
      "0 :  [0.16316735060948842, 0.00014425959167043478, 0.3904644305021977]\n",
      "1 :  [0.014213672601983567, 0.9996639715835126, 0.47876358605005614]\n",
      "2 :  [0.6200352427362672, 9.560019877348028e-05, 0.1944966697875451]\n",
      "3 :  [0.297925583682853, 0.2852954671040191, 6.111345559738201e-05]\n",
      "4 :  [0.1065169627344937, 0.6605221934939743, 0.9996305015453963]\n",
      "5 :  [0.2984878002345916, 0.0002541103104380782, 0.9988352518463963]\n",
      "6 :  [6.141171779432333e-05, 0.4516118388474953, 0.2096608852905208]\n",
      "7 :  [0.9998375366812359, 0.9998103475714253, 0.39396028930475274]\n",
      "8 :  [0.9602856117339487, 0.448814941575886, 0.0020854661599190223]\n",
      "9 :  [0.9998580602945267, 0.4825315473784935, 0.9998431183604863]\n",
      "10 :  [0.7346597975050703, 0.0008266610111419336, 0.9995091895887632]\n",
      "11 :  [0.6745025175255442, 0.9977305198575706, 0.00028415427872303863]\n",
      "12 :  [6.780681306170056e-05, 0.24597379119728666, 0.7442004862774253]\n",
      "13 :  [0.5510219627239428, 0.9999025217816504, 0.9997786574081519]\n",
      "14 :  [0.9998379083228425, 0.0701574193798476, 0.48769631769593874]\n",
      "15 :  [0.23270138393240583, 0.8750005193235402, 0.0012583983268205584]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [0, 0, 0]\n",
      "1 :  [0, 1, 0]\n",
      "2 :  [1, 0, 0]\n",
      "3 :  [0, 0, 0]\n",
      "4 :  [0, 1, 1]\n",
      "5 :  [0, 0, 1]\n",
      "6 :  [0, 0, 0]\n",
      "7 :  [1, 1, 0]\n",
      "8 :  [1, 0, 0]\n",
      "9 :  [1, 0, 1]\n",
      "10 :  [1, 0, 1]\n",
      "11 :  [1, 1, 0]\n",
      "12 :  [0, 0, 1]\n",
      "13 :  [1, 1, 1]\n",
      "14 :  [1, 0, 0]\n",
      "15 :  [0, 1, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 0]', '[1, 0, 1]', '[0, 0, 1]', '[0, 0, 0]', '[1, 1, 0]', '[0, 1, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  4 :  score is  1.0  ; loss is  0.208301103684\n",
      "hidden layer output:\n",
      "0 :  [0.4805860445873323, 0.9997548933764251, 0.9997636952777298]\n",
      "1 :  [0.00029836046113130047, 0.9993052364304917, 0.5748723106253655]\n",
      "2 :  [0.00023091570128219363, 0.7668248289926867, 0.11758537375042581]\n",
      "3 :  [0.11193700414561536, 0.29816469478523877, 0.0002193580844203047]\n",
      "4 :  [0.43644049274972313, 0.9994603260915703, 0.00016998259182076617]\n",
      "5 :  [0.06649183069056433, 0.0004189480811155314, 0.33463948883704153]\n",
      "6 :  [0.9998557614159516, 0.28159275299500136, 0.3262017211253338]\n",
      "7 :  [0.9997616101796373, 0.999771726079731, 0.6546581041211689]\n",
      "8 :  [0.6701065511987405, 0.39011155785816093, 4.47834701587916e-05]\n",
      "9 :  [0.3537984502316343, 0.0002676970828806239, 0.9951140994936859]\n",
      "10 :  [0.9998044089184641, 0.4145842220195671, 0.9998608389302707]\n",
      "11 :  [0.5183777250490718, 7.239537669643696e-05, 0.19812128185035566]\n",
      "12 :  [0.7909664725051446, 0.00029134311713932277, 0.708858118557221]\n",
      "13 :  [7.649802677702264e-05, 0.18121110494587853, 0.7409332272891684]\n",
      "14 :  [0.9511449564566811, 0.830896245586858, 0.17145127823296796]\n",
      "15 :  [0.11016443881135564, 0.5597750149660975, 0.9997749562994352]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [0, 1, 1]\n",
      "1 :  [0, 1, 1]\n",
      "2 :  [0, 1, 0]\n",
      "3 :  [0, 0, 0]\n",
      "4 :  [0, 1, 0]\n",
      "5 :  [0, 0, 0]\n",
      "6 :  [1, 0, 0]\n",
      "7 :  [1, 1, 1]\n",
      "8 :  [1, 0, 0]\n",
      "9 :  [0, 0, 1]\n",
      "10 :  [1, 0, 1]\n",
      "11 :  [1, 0, 0]\n",
      "12 :  [1, 0, 1]\n",
      "13 :  [0, 0, 1]\n",
      "14 :  [1, 1, 0]\n",
      "15 :  [0, 1, 1]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 0]', '[1, 0, 1]', '[0, 0, 1]', '[0, 0, 0]', '[0, 1, 1]', '[0, 1, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  5 :  score is  1.0  ; loss is  0.218320884064\n",
      "hidden layer output:\n",
      "0 :  [0.2735023897610499, 0.00011986844691223949, 0.997064300560519]\n",
      "1 :  [0.9997313615004495, 0.9996563933144498, 0.38380750504622413]\n",
      "2 :  [0.0008294542341492905, 0.00022798283268011843, 0.4417092856410469]\n",
      "3 :  [0.1218654980866567, 0.7571061645686544, 0.0002086582988638928]\n",
      "4 :  [0.0006162463718509062, 0.9997799104551134, 0.4249037438452487]\n",
      "5 :  [0.2893203539211506, 0.00017466851678058225, 9.25818486061051e-05]\n",
      "6 :  [0.0014704836461479694, 0.762348913901255, 0.9987990412955158]\n",
      "7 :  [0.9998723692079445, 0.05035302792137399, 0.465608325536944]\n",
      "8 :  [0.5944915860438994, 0.9995599152526183, 0.00014479728261956819]\n",
      "9 :  [0.7361911810721793, 0.0009678696845458965, 0.0525174878887499]\n",
      "10 :  [0.00010988622282391277, 0.2708084272967926, 0.9991105461190384]\n",
      "11 :  [0.5347936474412178, 0.9998801157619153, 0.9998451077457281]\n",
      "12 :  [0.9995945450917288, 0.5105158073253299, 0.0004583499009572636]\n",
      "13 :  [0.7345956127617986, 0.00036190001601654184, 0.9993242301130079]\n",
      "14 :  [0.9998637584140271, 0.5148217973838053, 0.9998417318800364]\n",
      "15 :  [9.946717472456618e-05, 0.30361151883816667, 0.048194902682374155]\n",
      "convert hidden layer output to 0/1:\n",
      "0 :  [0, 0, 1]\n",
      "1 :  [1, 1, 0]\n",
      "2 :  [0, 0, 0]\n",
      "3 :  [0, 1, 0]\n",
      "4 :  [0, 1, 0]\n",
      "5 :  [0, 0, 0]\n",
      "6 :  [0, 1, 1]\n",
      "7 :  [1, 0, 0]\n",
      "8 :  [1, 1, 0]\n",
      "9 :  [1, 0, 0]\n",
      "10 :  [0, 0, 1]\n",
      "11 :  [1, 1, 1]\n",
      "12 :  [1, 1, 0]\n",
      "13 :  [1, 0, 1]\n",
      "14 :  [1, 1, 1]\n",
      "15 :  [0, 0, 0]\n",
      "Failed to represent 16 numbers using different binary code in the hidden layer.\n",
      "Duplicates: ['[1, 0, 0]', '[1, 1, 1]', '[0, 0, 1]', '[0, 0, 0]', '[1, 1, 0]', '[0, 1, 0]']\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare((3,) , 'logistic', True, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With learning rate set as 0.01, all 5 runs for NN(3 perceptrons in hidden layer) converges and gives very good training accuracy and very low loss. The final output is the same as the input for all five runs with 1.0 accuracy. The final outputs are very stable. \n",
    "\n",
    "The original hidden layer outputs are a set of real numbers in the range of 0 to 1. For each number, I calculated 3 corresponding states of hidden layer nodes after the algorithm converges, as shown in above section. For example, in run 1, I got [0.9997714609328417, 0.34513298724656155, 0.9997484394145042] to represent 0 in the hidden layer. We could see these real numbers are close to 1 or 0 with quite a few exceptions.\n",
    "\n",
    "For comparing simplicity, we set a threshold of 0.5 to convert these real numbers into 0 or 1, i.e., any number bigger than 0.5 set as 1 and 0 otherwise. This would gives us the binary represenation of the input number in the hidden layer.\n",
    "\n",
    "Now we could clearly see that inside each run, we achieves some binary representation of each number in the hidden layer. However, the hidden layer fails to represent all 16 numbers, because some binary code is used to represent mulitple numbers. The case is worse than the one with 4 perceptrons or 5 perceptrons, which make sense since we could maximally represent 8 number with 3 perceptrons. For example, in run 1, [1, 0, 0] represented both 1 and 11. This happens to all 5 runs. The algorithm still converges due the fact that in practice we used real number rather than binary number in hidden layer. \n",
    "\n",
    "Between different runs, since we start with different intial weights, the final binary representation in hidden layer for a specific number may change. And it is clearly shown that lot of difference exists between runs regarding the binary representation in hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 5-layer NN. The 1st, 2nd, and 3rd hidden layers have 8, 4, and 8 perceptrons, respectively using sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random intialization  1 :  score is  0.0  ; loss is  3.75071070166\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n",
      "random intialization  2 :  score is  0.0  ; loss is  3.75025456915\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n",
      "random intialization  3 :  score is  0.0  ; loss is  3.75150465524\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n",
      "random intialization  4 :  score is  0.0  ; loss is  3.75167608116\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n",
      "random intialization  5 :  score is  0.0  ; loss is  3.74957118875\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare((8,4,8), 'logistic', False, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 5-layer NN. The 1st, 2nd, and 3rd hidden layers have 8, 4, and 8 perceptrons, respectively using relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random intialization  1 :  score is  1.0  ; loss is  0.0505047582769\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  2 :  score is  0.0  ; loss is  3.74048742646\n",
      "final output\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Accuracy:  0.0\n",
      "\n",
      "random intialization  3 :  score is  1.0  ; loss is  0.0613603674377\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  4 :  score is  1.0  ; loss is  0.0508279298593\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "random intialization  5 :  score is  1.0  ; loss is  0.039834539451\n",
      "final output\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Accuracy:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare((8,4,8), 'relu', False, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "I cannot get stable states with 5-layer NN using 'sigmoid' function as the activation function. The output and loss indicated that all five runs failed to converge to a local minimum. This is possibly due to the vanishing gradient problem. As we know, each of the neural network's weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. The sigmoid function g(z) = 1 / (1 + e^(-z)) have gradient in the range (0,1), and the back propogation computes gradients by the chain rule.This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly. Since we have multiple layers in the neural networks, it could occur this problem.\n",
    "\n",
    "However, if I used 'relu' function as the activation function, four out of five runs successfully convered to a local minimum with 100% accuracy. There is no vanishing or exploding gradient problems in relu function (f(x)= max(0,x)). So we could converge to a local minimum. The run 2 failed possibly because the algorithm stuck at some other local minumin with this random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
